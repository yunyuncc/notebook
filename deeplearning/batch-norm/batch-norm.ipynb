{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 批规范化(batch norm)是一种用来减少训练过程中的梯度消失的问题的技术。\n",
    "\n",
    "对应pytorch中的nn.BatchNorm2d层，一般添加在卷基层和激活层中间。将卷积输出的结果进行normalize,使每个channel中的数据分布符合均值为0,拥有单位方差。这样大部分数据都会分布在激活函数的敏感区间(以tanh为例就是在-1~1之间)。<br>\n",
    "这和数据预处理中的normalize是一样的道理，只不过数据预处理是处理输入层，而batch normalize是以mini batch为单位处理隐藏层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch norm 实现细节\n",
    "见下面的图和代码，计算的是所有batch中同一channel中所有数据的均值和方差<br>\n",
    "图源：https://www.cnblogs.com/yongjieShi/p/9332655.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](img/bn.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input channel0 mean: tensor(3.5000)\n",
      "input channel1 mean: tensor(3.3750)\n",
      "input channel2 mean: tensor(4.)\n",
      "input channel0 var: tensor(8.8000)\n",
      "input channel1 var: tensor(5.3167)\n",
      "input channel2 var: tensor(10.)\n",
      "The output mean value of the BN layer is 3.500000, 3.375000, 4.000000\n",
      "The output var value of the BN layer is 8.800000, 5.316667, 10.000000\n",
      "output channel0 mean: tensor(-0.0000, grad_fn=<MeanBackward1>)\n",
      "output channel1 mean: tensor(0., grad_fn=<MeanBackward1>)\n",
      "output channel2 mean: tensor(-0.0000, grad_fn=<MeanBackward1>)\n",
      "output channel0 var: tensor(0.0275, grad_fn=<VarBackward0>)\n",
      "output channel1 var: tensor(0.0003, grad_fn=<VarBackward0>)\n",
      "output channel2 var: tensor(1.0406, grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "m = nn.BatchNorm2d(3,momentum=1)\n",
    "input = torch.randint(0,10,(4,3,2,2)).float()\n",
    "\n",
    "print(\"input channel0 mean:\", input[:,0].mean())\n",
    "print(\"input channel1 mean:\", input[:,1].mean())\n",
    "print(\"input channel2 mean:\", input[:,2].mean())\n",
    "print(\"input channel0 var:\", input[:,0].var())\n",
    "print(\"input channel1 var:\", input[:,1].var())\n",
    "print(\"input channel2 var:\", input[:,2].var())\n",
    "output = m(input)\n",
    "assert(m.running_mean[0] == input[:,0].mean())\n",
    "assert(m.running_mean[1] == input[:,1].mean())\n",
    "assert(m.running_mean[2] == input[:,2].mean())\n",
    "\n",
    "assert(m.running_var[0] == input[:,0].var())\n",
    "assert(m.running_var[1] == input[:,1].var())\n",
    "assert(m.running_var[2] == input[:,2].var())\n",
    "print('The output mean value of the BN layer is %f, %f, %f' \n",
    "    % (m.running_mean[0],m.running_mean[1],m.running_mean[2]))\n",
    "print('The output var value of the BN layer is %f, %f, %f' \n",
    "    % (m.running_var[0],m.running_var[1],m.running_var[2]))\n",
    "\n",
    "print(\"output channel0 mean:\", output[:,0].mean())\n",
    "print(\"output channel1 mean:\", output[:,1].mean())\n",
    "print(\"output channel2 mean:\", output[:,2].mean())\n",
    "print(\"output channel0 var:\", output[:,0].var())\n",
    "print(\"output channel1 var:\", output[:,1].var())\n",
    "print(\"output channel2 var:\", output[:,2].var())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
